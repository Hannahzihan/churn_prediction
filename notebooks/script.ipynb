{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "labeled_path = os.path.join(parent_dir, \"data\", \"data_final\",\"labeled_data.csv\")\n",
    "labeled_data = pd.read_csv(labeled_path)\n",
    "# unlabeled_path = os.path.join(parent_dir, \"data\", \"data_final\",\"unlabeled_data.csv\")\n",
    "# unlabeled_data = pd.read_csv(unlabeled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_user_data(df, is_train=True, scaler=None, label_encoders=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop index and ID columns\n",
    "    for col in ['msno', 'registration_init_time']:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=col, inplace=True)\n",
    "\n",
    "    # Convert date columns to datetime\n",
    "    for col in ['last_login_date_previous', 'last_login_date_current']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    if all(c in df.columns for c in ['last_login_date_current', 'last_login_date_previous']):\n",
    "        default_date = pd.to_datetime(\"1970-01-01\")\n",
    "\n",
    "        # Normal calculation\n",
    "        df['days_until_month_end'] = ((df['last_login_date_current'] + pd.offsets.MonthEnd(0)) - df['last_login_date_current']).dt.days\n",
    "        df['last_login_interval'] = (df['last_login_date_current'] - df['last_login_date_previous']).dt.days\n",
    "\n",
    "        # Masks\n",
    "        curr_is_default = df['last_login_date_current'] == default_date\n",
    "        prev_is_default = df['last_login_date_previous'] == default_date\n",
    "        both_default = curr_is_default & prev_is_default\n",
    "\n",
    "        # Case 1: current is default\n",
    "        df.loc[curr_is_default & ~both_default, 'days_until_month_end'] = 45\n",
    "        df.loc[curr_is_default & ~both_default, 'last_login_interval'] = df.loc[curr_is_default & ~both_default].apply(\n",
    "            lambda row: 45 + (calendar.monthrange(row['last_login_date_previous'].year, row['last_login_date_previous'].month)[1] - row['last_login_date_previous'].day)\n",
    "            if row['last_login_date_previous'] != default_date else 90,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Case 2: previous is default\n",
    "        df.loc[prev_is_default & ~both_default, 'days_until_month_end'] = 45\n",
    "        df.loc[prev_is_default & ~both_default, 'last_login_interval'] = df.loc[prev_is_default & ~both_default].apply(\n",
    "            lambda row: 45 + calendar.monthrange(row['last_login_date_current'].year, row['last_login_date_current'].month)[1]\n",
    "            if row['last_login_date_current'] != default_date else 90,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Case 3: both are default\n",
    "        df.loc[both_default, 'days_until_month_end'] = 90\n",
    "        df.loc[both_default, 'last_login_interval'] = 90\n",
    "\n",
    "        # Drop original date columns\n",
    "        for col in ['last_login_date_previous', 'last_login_date_current', 'registration_init_time']:\n",
    "            if col in df.columns:\n",
    "                df.drop(columns=col, inplace=True)\n",
    "\n",
    "    # Label encode categorical features\n",
    "    cat_cols = ['gender', 'city', 'registered_via']\n",
    "    fitted_label_encoders = {}\n",
    "\n",
    "    for col in cat_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            if is_train:\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col])\n",
    "                fitted_label_encoders[col] = le\n",
    "            else:\n",
    "                le = label_encoders[col]\n",
    "                df[col] = le.transform(df[col])\n",
    "\n",
    "    # Separate target if training\n",
    "    if is_train:\n",
    "        y = df['is_churn'].astype(int)\n",
    "        df.drop(columns=['is_churn'], inplace=True)\n",
    "    else:\n",
    "        y = None\n",
    "        df.drop(columns=['is_churn'], inplace=True)\n",
    "\n",
    "    # Standardize numerical features\n",
    "    X = df.copy()\n",
    "    if is_train:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = scaler.transform(X)\n",
    "\n",
    "    X_scaled = df.copy()\n",
    "    \n",
    "    if is_train:\n",
    "        return X_scaled, y, scaler, fitted_label_encoders\n",
    "    else:\n",
    "        return X_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labled, y, scaler, fitted_label_encoders = preprocess_user_data(labeled_data, is_train=True, scaler=None, label_encoders=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Example Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_labled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_evaluate, y_train, y_evaluate = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE + Downsampling\n",
    "\n",
    "def resample_smote_then_downsample(X, y, downsample_ratio=1.0):\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = sm.fit_resample(X, y)\n",
    "    df = pd.DataFrame(X_smote)\n",
    "    df['label'] = y_smote\n",
    "    majority = df[df['label'] == 0]\n",
    "    minority = df[df['label'] == 1]\n",
    "    majority_down = majority.sample(n=int(len(minority) * downsample_ratio), random_state=42)\n",
    "    df_resampled = pd.concat([majority_down, minority], axis=0).sample(frac=1, random_state=42)\n",
    "    return df_resampled.drop(columns='label'), df_resampled['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% MLP Classifier\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_rate=0.3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(last_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            last_dim = h_dim\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train_model(model, train_loader, val_loader, threshold=0.5,\n",
    "                epochs=30, lr=0.001, patience=5, device='cpu', return_logs=False):\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    no_improve_epochs = 0\n",
    "    logs = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device).float()\n",
    "            y_batch = y_batch.to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val = X_val.to(device).float()\n",
    "                y_val = y_val.to(device).float()\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_losses.append(loss.item())\n",
    "                preds = (outputs >= threshold).int().cpu().numpy()\n",
    "                y_pred.extend(preds)\n",
    "                y_true.extend(y_val.cpu().numpy())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        logs['train_loss'].append(np.mean(train_losses))\n",
    "        logs['val_loss'].append(val_loss)\n",
    "        logs['val_f1'].append(f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {logs['train_loss'][-1]:.4f} | Val Loss: {val_loss:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return (model, logs) if return_logs else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search with K-Fold\n",
    "\n",
    "def run_random_search(X, y, param_grid, k=5, epochs=30, batch_size=256, patience=5, device='cpu'):\n",
    "    best_params = None\n",
    "    best_avg_f1 = -1\n",
    "    results = []\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    save_path = \"best_mlp_model.pt\"\n",
    "    os.makedirs(os.path.dirname(save_path) or '.', exist_ok=True)\n",
    "\n",
    "    for params in param_grid:\n",
    "        hidden_dims = params['hidden_dims']\n",
    "        lr = params['lr']\n",
    "        dropout = 0.3\n",
    "        threshold = 0.5\n",
    "        fold_f1_scores = []\n",
    "\n",
    "        print(f\"\\nTesting params: hidden_dims={hidden_dims}, lr={lr}, dropout={dropout}, threshold={threshold}\")\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            X_tr_bal, y_tr_bal = resample_smote_then_downsample(X_tr, y_tr)\n",
    "\n",
    "            train_dataset = TensorDataset(torch.tensor(X_tr_bal.values, dtype=torch.float32),\n",
    "                                          torch.tensor(y_tr_bal.values, dtype=torch.float32))\n",
    "            val_dataset = TensorDataset(torch.tensor(X_val.values, dtype=torch.float32),\n",
    "                                        torch.tensor(y_val.values, dtype=torch.float32))\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = MLPClassifier(input_dim=X.shape[1], hidden_dims=hidden_dims, dropout_rate=dropout)\n",
    "            model, logs = train_model(model, train_loader, val_loader,\n",
    "                                      threshold=threshold,\n",
    "                                      epochs=epochs, lr=lr, patience=patience,\n",
    "                                      device=device, return_logs=True)\n",
    "\n",
    "            model.eval()\n",
    "            y_pred = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, _ in val_loader:\n",
    "                    X_batch = X_batch.to(device).float()\n",
    "                    outputs = model(X_batch)\n",
    "                    preds = (outputs >= threshold).int().cpu().numpy()\n",
    "                    y_pred.extend(preds)\n",
    "\n",
    "            y_val_np = y_val.values\n",
    "            acc = accuracy_score(y_val_np, y_pred)\n",
    "            prec = precision_score(y_val_np, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_val_np, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_val_np, y_pred)\n",
    "            cm = confusion_matrix(y_val_np, y_pred)\n",
    "\n",
    "            fold_f1_scores.append(f1)\n",
    "            print(f\"  Fold {fold+1} - Acc: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "            print(\"  Confusion Matrix:\")\n",
    "            print(cm)\n",
    "\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(logs['train_loss'], label='Train Loss')\n",
    "            plt.plot(logs['val_loss'], label='Val Loss')\n",
    "            plt.plot(logs['val_f1'], label='Val F1')\n",
    "            plt.title(f\"Fold {fold + 1} Training Curve\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss / F1\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        avg_f1 = np.mean(fold_f1_scores)\n",
    "        results.append((params, avg_f1))\n",
    "        if avg_f1 > best_avg_f1:\n",
    "            best_avg_f1 = avg_f1\n",
    "            best_params = params\n",
    "            torch.save(model.state_dict(), save_path)  # ✅ 保存最优模型\n",
    "\n",
    "    print(f\"\\nBest Params: {best_params} | Best Avg F1: {best_avg_f1:.4f}\")\n",
    "    return best_params, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert evaluation data to tensor\n",
    "X_eval_tensor = torch.tensor(X_evaluate.values, dtype=torch.float32)\n",
    "y_eval_tensor = torch.tensor(y_evaluate.values, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader (no shuffle needed for evaluation)\n",
    "eval_dataset = TensorDataset(X_eval_tensor, y_eval_tensor)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing params: hidden_dims=[512, 256], lr=0.0005, dropout=0.4, threshold=0.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m generate_random_param_grid(\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m     19\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m resample_smote_then_downsample(X_train, y_train)\n\u001b[1;32m---> 20\u001b[0m best_params, results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_random_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mrun_random_search\u001b[1;34m(X, y, param_grid, k, epochs, batch_size, patience, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m X_tr, X_val \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[train_idx], X\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[0;32m     23\u001b[0m y_tr, y_val \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_idx], y\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m---> 24\u001b[0m X_tr_bal, y_tr_bal \u001b[38;5;241m=\u001b[39m \u001b[43mresample_smote_then_downsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(X_tr_bal\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     27\u001b[0m                               torch\u001b[38;5;241m.\u001b[39mtensor(y_tr_bal\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     28\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(X_val\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     29\u001b[0m                             torch\u001b[38;5;241m.\u001b[39mtensor(y_val\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mresample_smote_then_downsample\u001b[1;34m(X, y, downsample_ratio)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresample_smote_then_downsample\u001b[39m(X, y, downsample_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[0;32m      4\u001b[0m     sm \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     X_smote, y_smote \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_smote)\n\u001b[0;32m      7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_smote\n",
      "File \u001b[1;32md:\\24fall\\lent term\\Project2\\MusicStreamingSubscriptionsChurnPredictor\\churn_env\\lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\24fall\\lent term\\Project2\\MusicStreamingSubscriptionsChurnPredictor\\churn_env\\lib\\site-packages\\imblearn\\base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32md:\\24fall\\lent term\\Project2\\MusicStreamingSubscriptionsChurnPredictor\\churn_env\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:389\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    386\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[1;32m--> 389\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_k_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    390\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[0;32m    391\u001b[0m     X_class, y\u001b[38;5;241m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    392\u001b[0m )\n\u001b[0;32m    393\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n",
      "File \u001b[1;32md:\\24fall\\lent term\\Project2\\MusicStreamingSubscriptionsChurnPredictor\\churn_env\\lib\\site-packages\\sklearn\\neighbors\\_base.py:869\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    862\u001b[0m use_pairwise_distances_reductions \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[0;32m    865\u001b[0m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_\n\u001b[0;32m    866\u001b[0m     )\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[0;32m    881\u001b[0m ):\n\u001b[0;32m    882\u001b[0m     results \u001b[38;5;241m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    883\u001b[0m         X, n_neighbors\u001b[38;5;241m=\u001b[39mn_neighbors, return_distance\u001b[38;5;241m=\u001b[39mreturn_distance\n\u001b[0;32m    884\u001b[0m     )\n",
      "File \u001b[1;32md:\\24fall\\lent term\\Project2\\MusicStreamingSubscriptionsChurnPredictor\\churn_env\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:281\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03mreturns.\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[0;32m    294\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    295\u001b[0m         Y\u001b[38;5;241m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    301\u001b[0m         return_distance\u001b[38;5;241m=\u001b[39mreturn_distance,\n\u001b[0;32m    302\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\\\metrics\\\\_pairwise_distances_reduction\\\\_argkmin.pyx:59\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\24fall\\lent term\\Project2\\MusicStreamingSubscriptionsChurnPredictor\\churn_env\\lib\\site-packages\\threadpoolctl.py:592\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_original_limits()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, \u001b[38;5;241m*\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#%% Generate Random Param Grid\n",
    "def generate_random_param_grid(n_samples=6):\n",
    "    hidden_dims_choices = [[128, 64], [512, 256], [256, 128, 64], [1024, 512, 256]]\n",
    "    lr_choices = [0.0001, 0.0005, 0.001]\n",
    "    grid = []\n",
    "    for _ in range(n_samples):\n",
    "        grid.append({\n",
    "            'hidden_dims': random.choice(hidden_dims_choices),\n",
    "            'lr': random.choice(lr_choices),\n",
    "        })\n",
    "    return grid\n",
    "\n",
    "#%% Run Random Search\n",
    "param_grid = generate_random_param_grid(6)\n",
    "X_resampled, y_resampled = resample_smote_then_downsample(X_train, y_train)\n",
    "best_params, results = run_random_search(\n",
    "    X_resampled, y_resampled,\n",
    "    param_grid=param_grid,\n",
    "    k=5,\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    patience=5,\n",
    "    device='cpu'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
